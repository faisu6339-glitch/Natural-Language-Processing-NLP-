{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKwcc1dK+h6yGwa1kHHQK2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faisu6339-glitch/Natural-Language-Processing-NLP-/blob/main/Removing_stopwords_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Removing stop words with NLTK in Python\n"
      ],
      "metadata": {
        "id": "k_hZhvyp4UrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural language processing tasks often involve filtering out commonly occurring words that provide no or very little semantic value to text analysis. These words are known as stopwords include articles, prepositions and pronouns like \"the\", \"and\", \"is\" and \"in\". While they seem insignificant, proper stopword handling can dramatically impact the performance and accuracy of NLP applications."
      ],
      "metadata": {
        "id": "DzxjAFlL4YSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the sentence: \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "#Stopwords:\n",
        "\"the\" and \"over\"\n",
        "Content words: \"quick\", \"brown\", \"fox\", \"jumps\", \"lazy\", \"dog\"\n",
        "It becomes particularly important when dealing with large text corpora where computational efficiency matters. Processing every single word including high-frequency stopwords can consume unnecessary resources and potentially skew analysis results.\n",
        "\n",
        "#When to Remove Stopwords\n",
        "The decision to remove stopwords depends heavily on the specific NLP task at hand:\n",
        "\n",
        "#Tasks that benefit from stopword removal:\n",
        "Text classification and sentiment analysis\n",
        "Information retrieval and search engines\n",
        "Topic modelling and clustering\n",
        "Keyword extraction\n",
        "Tasks that require preserving stopwords:\n",
        "Machine translation (maintains grammatical structure)\n",
        "Text summarization (preserves sentence coherence)\n",
        "Question-answering systems (syntactic relationships matter)\n",
        "Grammar checking and parsing\n",
        "Language modeling presents an interesting middle ground where the decision depends on the specific application requirements and available computational resources.\n",
        "\n",
        "#Categories of Stopwords\n",
        "Understanding different types of stopwords helps in making informed decisions:\n",
        "\n",
        "Standard Stopwords: Common function words like articles (\"a\", \"the\"), conjunctions (\"and\", \"but\") and prepositions (\"in\", \"on\")\n",
        "Domain-Specific Stopwords: Context-dependent terms that appear frequently in specific fields like \"patient\" in medical texts\n",
        "Contextual Stopwords: Words with extremely high frequency in particular datasets\n",
        "\n",
        "Numerical Stopwords: Digits, punctuation marks and single characters\n",
        "Implementation with NLTK\n",
        "NLTK provides robust support for stopword removal across 16 different languages. The implementation involves tokenization followed by filtering:\n",
        "\n",
        "Setup: Import NLTK modules and download required resources like stopwords and tokenizer data.\n",
        "Text preprocessing: Convert the sample sentence to lowercase and tokenize it into words.\n",
        "Stopword removal: Load English stopwords and filter them out from the token list.\n",
        "Output: Print both the original and cleaned tokens for comparison."
      ],
      "metadata": {
        "id": "WKJaE12O45zx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ00mKUZ4OX5",
        "outputId": "0634f686-64dd-4cdd-bb1a-9c1e8c4b861b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ['this', 'is', 'a', 'sample', 'sentence', 'showing', 'stopword', 'removal', '.']\n",
            "Filtered: ['sample', 'sentence', 'showing', 'stopword', 'removal', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Sample text\n",
        "text = \"This is a sample sentence showing stopword removal.\"\n",
        "\n",
        "# Get English stopwords and tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "+\n",
        "print(\"Original:\", tokens)\n",
        "print(\"Filtered:\", filtered_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation with Scikit Learn\n"
      ],
      "metadata": {
        "id": "7ZIxe6njCbic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Another sample text\n",
        "new_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the new text using NLTK\n",
        "new_words = word_tokenize(new_text)\n",
        "\n",
        "# Remove stopwords using NLTK\n",
        "new_filtered_words = [\n",
        "    word for word in new_words if word.lower() not in stopwords.words('english')]\n",
        "\n",
        "# Join the filtered words to form a clean text\n",
        "new_clean_text = ' '.join(new_filtered_words)\n",
        "\n",
        "print(\"Original Text:\", new_text)\n",
        "print(\"Text after Stopword Removal:\", new_clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVBHAkZ15l2F",
        "outputId": "d69d4eed-a34d-45e6-a469-f87f0db4d28d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The quick brown fox jumps over the lazy dog.\n",
            "Text after Stopword Removal: quick brown fox jumps lazy dog .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# import nltk for stopwords\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "\n",
        "# assign string\n",
        "no_wspace_string='python  released in  was a major revision of the language that is not completely backward compatible and much python  code does not run unmodified on python  with python s endoflife only python x and later are supported with older versions still supporting eg windows  and old installers not restricted to bit windows'\n",
        "\n",
        "# convert string to list of words\n",
        "lst_string = [no_wspace_string][0].split()\n",
        "print(lst_string)\n",
        "\n",
        "# remove stopwords\n",
        "no_stpwords_string=\"\"\n",
        "for i in lst_string:\n",
        "    if not i in stop_words:\n",
        "        no_stpwords_string += i+' '\n",
        "\n",
        "# removing last space\n",
        "no_stpwords_string = no_stpwords_string[:-1]\n",
        "print(no_stpwords_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wcq1BcTCfkI",
        "outputId": "bbd7eab8-32f9-4011-8972-866ade4b6676"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'both', \"needn't\", 'any', 'doing', 'nor', \"should've\", \"couldn't\", \"we're\", 'such', \"i'll\", 'd', 'more', \"isn't\", \"i'm\", 'am', 'do', 'those', 'as', 'between', \"he's\", 'had', \"it's\", \"shan't\", 'been', 'themselves', \"wasn't\", 'yours', 'himself', \"she's\", 'it', 'does', 'm', 'weren', \"we'll\", 'shan', 'yourselves', 'who', 'are', 'your', \"weren't\", \"you've\", \"you'd\", 'didn', 'few', 'if', 'just', 'you', 't', 'up', \"we'd\", 'its', 's', \"that'll\", 'is', 'ours', \"he'd\", 'a', 'above', 'while', 'for', 'o', 'they', \"she'll\", 'where', \"they'd\", 'these', 'we', 'once', \"didn't\", 'own', 'than', \"he'll\", 'can', \"doesn't\", \"hadn't\", 'her', 'that', \"i'd\", 'were', \"we've\", 'wouldn', 'off', 'being', 'because', 'but', 'what', 'whom', 'after', 're', 'couldn', \"haven't\", 'shouldn', 'and', \"you'll\", 'should', 'below', 'same', 'wasn', 'ourselves', 'ain', 'most', 'herself', 'with', 'from', \"they'll\", \"she'd\", 'this', 'into', 'all', 'when', 'doesn', \"they've\", 'too', 'no', 'to', 'very', 'in', 'of', 'did', 'during', 'the', \"shouldn't\", 'against', 'our', 'he', \"won't\", 'or', 'further', 'about', 'an', 'before', \"mustn't\", 'haven', 'down', 'she', 'some', 'so', 'them', \"mightn't\", 'has', 'then', 'by', \"it'd\", 'through', 'at', 'having', 'their', 'be', 'isn', 'my', \"aren't\", \"wouldn't\", 'again', 'hadn', 'here', 'ma', 'was', \"you're\", 'over', \"they're\", 'until', 'under', \"hasn't\", 'don', 've', 'now', 'needn', 'me', 'myself', 'itself', 'each', 'aren', 'why', 'won', \"i've\", \"don't\", 'out', 'which', 'not', 'only', \"it'll\", 'mustn', 'on', 'his', 'hers', 'hasn', 'll', 'other', 'there', 'theirs', 'how', 'will', 'yourself', 'y', 'him', 'i', 'mightn', 'have'}\n",
            "['python', 'released', 'in', 'was', 'a', 'major', 'revision', 'of', 'the', 'language', 'that', 'is', 'not', 'completely', 'backward', 'compatible', 'and', 'much', 'python', 'code', 'does', 'not', 'run', 'unmodified', 'on', 'python', 'with', 'python', 's', 'endoflife', 'only', 'python', 'x', 'and', 'later', 'are', 'supported', 'with', 'older', 'versions', 'still', 'supporting', 'eg', 'windows', 'and', 'old', 'installers', 'not', 'restricted', 'to', 'bit', 'windows']\n",
            "python released major revision language completely backward compatible much python code run unmodified python python endoflife python x later supported older versions still supporting eg windows old installers restricted bit windows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0ZUK42CCy9h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}