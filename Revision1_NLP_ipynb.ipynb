{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1NIq961dpprevEWhIoTYk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faisu6339-glitch/Natural-Language-Processing-NLP-/blob/main/Revision1_NLP_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "xCyk6CQVTunz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BmfP3TwQUFh6"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b56d88e4"
      },
      "source": [
        "NLTK stands for Natural Language Toolkit. It's a powerful and popular open-source library in Python for working with human language data (text). It provides easy-to-use interfaces to over 50 corpora and lexical resources, such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
        "\n",
        "Here's a breakdown of what NLTK is and what it's used for:\n",
        "\n",
        "What is NLTK?\n",
        "\n",
        "Python Library: It's primarily a Python library, making it accessible and easy to integrate into Python projects.\n",
        "Open Source: It's freely available and has a large, active community.\n",
        "Comprehensive: It offers a wide range of tools and resources for various NLP tasks, from basic text processing to more advanced linguistic analysis.\n",
        "Educational Tool: It's widely used in academia for teaching and research in NLP, computational linguistics, and artificial intelligence.\n",
        "Prototyping: It's excellent for rapid prototyping and experimenting with different NLP techniques before moving to more specialized or performance-oriented libraries for production.\n",
        "Key Features and Functionalities:\n",
        "\n",
        "Tokenization: Breaking down text into smaller units (words, sentences).\n",
        "\n",
        "nltk.word_tokenize(): Splits text into words.\n",
        "nltk.sent_tokenize(): Splits text into sentences.\n",
        "Stemming and Lemmatization: Reducing words to their base or root form.\n",
        "\n",
        "Stemming: Removes suffixes to get to a base form (e.g., \"running\" -> \"run\", \"generously\" -> \"generous\"). NLTK includes stemmers like PorterStemmer and SnowballStemmer.\n",
        "Lemmatization: Reduces words to their dictionary form (lemma) using vocabulary and morphological analysis (e.g., \"better\" -> \"good\", \"ran\" -> \"run\"). NLTK uses WordNetLemmatizer.\n",
        "Stop Words: Removing common words (like \"the,\" \"is,\" and \"nltk.pos_t\") that often don't carry significant meaning for analysis.\n",
        "\n",
        "Part-of-Speech (POS) Tagging: Identifying the grammatical role of each word in a sentence (e.g., noun, verb, adjective)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a3dd281"
      },
      "source": [
        "### Stemming\n",
        "Stemming is a rule-based text normalisation technique that reduces words to their root form by removing prefixes or suffixes. The resulting form called a stem, may not be a valid or meaningful word in the language.\n",
        "\n",
        "Each word is processed independently without considering context\n",
        "The algorithm checks for common suffixes or prefixes\n",
        "Predefined heuristic rules are applied to strip these affixes\n",
        "The remaining part of the word is returned as the stem\n",
        "No grammatical or semantic validation is performed\n",
        "In essence, stemming performs mechanical truncation of words.\n",
        "\n",
        "#### Techniques Used\n",
        "Suffix Stripping: Removes common endings like -ing, -ed, -es\n",
        "Rule-Based Truncation: Applies fixed linguistic rules\n",
        "Aggressive Reduction: Shortens words for maximum generalization\n",
        "\n",
        "#### Example:\n",
        "\n",
        "| Original Word | Stem |\n",
        "|---------------|------|\n",
        "| running       | run  |\n",
        "| studies       | studi|\n",
        "| smiling       | smile|\n",
        "| communication | commun|"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "ikrQVwgdU2IM"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"Hello Aaliya Fatma, I miss you from the deepest depths of my heart.\"\n"
      ],
      "metadata": {
        "id": "lsQRea7KU3ed"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "O9zIgWc_VNRP",
        "outputId": "6983afd7-ec93-41bd-bd8c-20d923b517fa"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello Aaliya Fatma, I miss you from the deepest depths of my heart.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt.split('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiZFj5VaVOF_",
        "outputId": "cee78e25-075b-4a77-fe58-62ad18814ecf"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello Aaliya Fatma, I miss you from the deepest depths of my heart', '']"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt.split(\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_cRKfp_VSwn",
        "outputId": "d4067ae6-d6f7-4826-8637-89ca0945746d"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Aaliya',\n",
              " 'Fatma,',\n",
              " 'I',\n",
              " 'miss',\n",
              " 'you',\n",
              " 'from',\n",
              " 'the',\n",
              " 'deepest',\n",
              " 'depths',\n",
              " 'of',\n",
              " 'my',\n",
              " 'heart.']"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt.split(' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6REu3tCeVVd-",
        "outputId": "825c4305-d1dd-478e-88be-cb17d7ed158d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Aaliya',\n",
              " 'Fatma,',\n",
              " 'I',\n",
              " 'miss',\n",
              " 'you',\n",
              " 'from',\n",
              " 'the',\n",
              " 'deepest',\n",
              " 'depths',\n",
              " 'of',\n",
              " 'my',\n",
              " 'heart.']"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(txt.split(' '))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLd1XpbNVZMi",
        "outputId": "10b75466-f2da-4925-c69b-d9a7742be1c8"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "uOVuodCjVewN"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LkBZLaToVkyB"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d45a87a8",
        "outputId": "2d5b20f9-ab52-4f68-ccee-31cefba1a0d5"
      },
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3442516f",
        "outputId": "03d00868-77d3-419e-a561-7a6165c12a7c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c09046d4",
        "outputId": "684e4d70-5688-45b7-fb80-61cb2f80778d"
      },
      "source": [
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "word_tokenize(txt)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Aaliya',\n",
              " 'Fatma',\n",
              " ',',\n",
              " 'I',\n",
              " 'miss',\n",
              " 'you',\n",
              " 'from',\n",
              " 'the',\n",
              " 'deepest',\n",
              " 'depths',\n",
              " 'of',\n",
              " 'my',\n",
              " 'heart',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eUakOQGV5D1",
        "outputId": "74e5cfef-e818-4529-f6d0-409aaf6bbd64"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello Aaliya Fatma, I miss you from the deepest depths of my heart.']"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_tokenize(txt):\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BVJYkj1V7Fx",
        "outputId": "1cd26d8c-6b84-47db-de94-24fddd01b496"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "Aaliya\n",
            "Fatma\n",
            ",\n",
            "I\n",
            "miss\n",
            "you\n",
            "from\n",
            "the\n",
            "deepest\n",
            "depths\n",
            "of\n",
            "my\n",
            "heart\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in sent_tokenize(txt):\n",
        "    if word.endswith('a'):\n",
        "        print(word)"
      ],
      "metadata": {
        "id": "D2l_0BT5WJAs"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qjivuzQBWoqz"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dc490ee"
      },
      "source": [
        "### NLTK Stemming Examples\n",
        "\n",
        "Let's apply the different stemming algorithms to a sample text to see how they work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7dd5e52",
        "outputId": "75f998f2-7757-4ec4-e7de-eb5f9139a2e1"
      },
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"The quick brown foxes are running quickly through the beautiful forest, connecting with nature.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = word_tokenize(text)\n",
        "print(f\"Original words: {words}\\n\")"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['The', 'quick', 'brown', 'foxes', 'are', 'running', 'quickly', 'through', 'the', 'beautiful', 'forest', ',', 'connecting', 'with', 'nature', '.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66321f60"
      },
      "source": [
        "#### 1. Porter Stemmer\n",
        "\n",
        "The Porter Stemmer is one of the oldest and most widely used. It's known for being fairly aggressive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79445290",
        "outputId": "d70391e9-ef30-4bee-d98d-14d14d7bc44b"
      },
      "source": [
        "porter = PorterStemmer()\n",
        "porter_stems = [porter.stem(word) for word in words]\n",
        "print(f\"Porter Stemmer: {porter_stems}\")"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer: ['the', 'quick', 'brown', 'fox', 'are', 'run', 'quickli', 'through', 'the', 'beauti', 'forest', ',', 'connect', 'with', 'natur', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02fa6560"
      },
      "source": [
        "#### 2. Snowball Stemmer (Porter2 Stemmer)\n",
        "\n",
        "The Snowball Stemmer is an improved version of the Porter Stemmer, offering better performance and supporting multiple languages. It's often less aggressive than the original Porter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f97dc1b7",
        "outputId": "989fdf32-a16c-46fc-edb3-2b540046eec9"
      },
      "source": [
        "# Specify the language for Snowball Stemmer (e.g., 'english')\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "snowball_stems = [snowball.stem(word) for word in words]\n",
        "print(f\"Snowball Stemmer: {snowball_stems}\")"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Snowball Stemmer: ['the', 'quick', 'brown', 'fox', 'are', 'run', 'quick', 'through', 'the', 'beauti', 'forest', ',', 'connect', 'with', 'natur', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bdca6c5"
      },
      "source": [
        "#### 3. Lancaster Stemmer\n",
        "\n",
        "The Lancaster Stemmer is generally the most aggressive of the three, often producing very short, sometimes unrecognizable stems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51866fd6",
        "outputId": "2c3a1752-1991-4f79-917b-4aed0651426a"
      },
      "source": [
        "lancaster = LancasterStemmer()\n",
        "lancaster_stems = [lancaster.stem(word) for word in words]\n",
        "print(f\"Lancaster Stemmer: {lancaster_stems}\")"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lancaster Stemmer: ['the', 'quick', 'brown', 'fox', 'ar', 'run', 'quick', 'through', 'the', 'beauty', 'forest', ',', 'connect', 'with', 'nat', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f5dc8e5"
      },
      "source": [
        "**Stemming Explained in Detail**\n",
        "\n",
        "Stemming is a text normalization technique that reduces words to their **root or base form**, often called a \"stem.\" The primary goal of stemming is to remove suffixes (and sometimes prefixes) from words so that different inflected forms of a word (e.g., \"running,\" \"runs,\" \"ran\") are mapped to a common base word (e.g., \"run\").\n",
        "\n",
        "**Key Characteristics of Stemming:**\n",
        "\n",
        "1.  **Rule-Based Heuristics:** Stemming algorithms typically use a set of heuristic rules to chop off the ends of words. These rules are often language-specific.\n",
        "2.  **Does Not Guarantee Lexical Correctness:** The resulting \"stem\" may not always be a valid word in the dictionary. For instance, the stem of \"beautiful\" might be \"beauti,\" which isn't a word. This is a crucial distinction from lemmatization.\n",
        "3.  **Faster and Simpler:** Stemmers are generally simpler and faster to implement and execute compared to lemmatizers, as they don't rely on lexical dictionaries or advanced morphological analysis.\n",
        "4.  **Reduces Dimensionality:** By mapping multiple forms of a word to a single stem, stemming helps in reducing the total number of unique words in a corpus. This is beneficial for tasks like information retrieval, text classification, and clustering, where you want to treat variations of a word as the same underlying concept.\n",
        "\n",
        "**Common Stemming Algorithms (in NLTK):**\n",
        "\n",
        "NLTK provides several popular stemming algorithms:\n",
        "\n",
        "1.  **Porter Stemmer:**\n",
        "    *   One of the oldest and most widely used stemmers, developed by Martin Porter in 1980.\n",
        "    *   It applies a series of rules (e.g., remove 's', 'es', 'ing', 'ed') in multiple passes.\n",
        "    *   It's known for being aggressive, meaning it can sometimes over-stem words.\n",
        "    *   *Example:* \"connection,\" \"connections,\" \"connected,\" \"connecting\" -> \"connect\"\n",
        "    *   *Example:* \"policy,\" \"policies\" -> \"polici\"\n",
        "\n",
        "2.  **Snowball Stemmer (Porter2 Stemmer):**\n",
        "    *   An improved version of the Porter Stemmer, also developed by Martin Porter.\n",
        "    *   It's more sophisticated and offers better performance for various languages.\n",
        "    *   It's less aggressive than the original Porter Stemmer in some cases, leading to better results.\n",
        "    *   *Example:* \"generously\" -> \"generous\"\n",
        "\n",
        "3.  **Lancaster Stemmer:**\n",
        "    *   More aggressive than both Porter and Snowball stemmers.\n",
        "    *   Often produces very short, sometimes unrecognizable stems.\n",
        "    *   While highly effective in reducing word forms, its aggressive nature can sometimes lead to loss of meaning or make the stems difficult to interpret.\n",
        "    *   *Example:* \"maximum,\" \"maximus,\" \"maximization\" -> \"maxim\"\n",
        "\n",
        "**When to Use Stemming:**\n",
        "\n",
        "*   **Information Retrieval:** When you want a search query for \"fishing\" to also return documents containing \"fishes\" or \"fished.\"\n",
        "*   **Text Classification:** To reduce the feature space (number of unique words) and group semantically similar words together, potentially improving model performance by treating different forms of a word as the same.\n",
        "*   **Sentiment Analysis:** To treat \"happy,\" \"happier,\" \"happiest\" as the same positive sentiment indicator.\n",
        "*   **Initial Data Exploration:** For quick and dirty text normalization when performance is critical, and a perfect dictionary word is not required.\n",
        "\n",
        "**Limitations of Stemming:**\n",
        "\n",
        "*   **Over-stemming:** Removing too much of a word, leading to loss of meaning or combining words that should be distinct (e.g., \"universal\" and \"university\" might both stem to \"univers\").\n",
        "*   **Under-stemming:** Failing to reduce words that should be mapped to the same stem (e.g., \"theory\" and \"theorize\" might not stem to the same root).\n",
        "*   **Produces Non-Words:** As mentioned, the output is not guaranteed to be a valid dictionary word.\n",
        "\n",
        "**How it differs from Lemmatization:**\n",
        "\n",
        "The main difference lies in the output: stemming is a heuristic process that chops off endings, often resulting in non-dictionary words, whereas lemmatization uses lexical knowledge (dictionaries and morphological analysis) to return the base **dictionary form** (lemma) of a word, which is always a valid word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9370770b",
        "outputId": "77a6cb02-7d69-4591-caae-ebd5897c6988"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1Ô∏è‚É£ Porter Stemmer ‚≠ê (Most Popular)"
      ],
      "metadata": {
        "id": "b8FI_v2WXlb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Program 1"
      ],
      "metadata": {
        "id": "dtKuXW50Yojj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps=PorterStemmer()\n",
        "ps.stem(\"running\")\n",
        "ps.stem(\"Aaliya Fatma\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oPkmIin7qBzA",
        "outputId": "968e3879-dd77-4228-b1c9-3b0ae50a4fa1"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'aaliya fatma'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "word1 = ps.stem(\"running\")\n",
        "word2 = ps.stem(\"Aaliya Fatma\")\n",
        "\n",
        "word1, word2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te2gZ-hrrH_9",
        "outputId": "24cfc393-421a-4539-8dae-963b5be8b280"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('run', 'aaliya fatma')"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "ps.stem(\"running\")\n",
        "ps.stem(\"studies\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aaeHAdMcWocS",
        "outputId": "caf40967-8488-48c2-eaef-d3a5094ece12"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'studi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FRwUGXkkqA9d"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program 2"
      ],
      "metadata": {
        "id": "XteLGdhJYmmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "words = [\"running\", \"runs\", \"runner\", \"studies\", \"easily\"]\n",
        "stems = [ps.stem(word) for word in words]\n",
        "\n",
        "print(stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AydpWBzuYCpc",
        "outputId": "edb4e7d7-97da-4dbb-de97-13c59d8caf74"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'run', 'runner', 'studi', 'easili']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "\n",
        "words=[\"Stronger\",\"Bigger\",\"Larger\",\"harder\",\"Easliy\",\"Luckily\",\"Connected\",\"Connections\"]\n",
        "stems=[ps.stem(word) for word in words]\n",
        "print(stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CD6YzcOnrQTO",
        "outputId": "ffedfe1a-31c4-4dbf-9d42-e1eda09c3ab5"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['stronger', 'bigger', 'larger', 'harder', 'easliy', 'luckili', 'connect', 'connect']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program 3"
      ],
      "metadata": {
        "id": "72R4TE9AYuw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "sentence = \"I was running and he runs every day\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "stemmed_words = [ps.stem(word) for word in tokens]\n",
        "\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pzmbkRhYG0U",
        "outputId": "89da2f1c-ead2-49eb-8479-c249c3252046"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'wa', 'run', 'and', 'he', 'run', 'everi', 'day']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "sentence=\"Dil mera tod Students are studying and learning machine learning techniques daily\"\n",
        "tokens=word_tokenize(sentence)\n",
        "\n",
        "stemmed_words=[ps.stem(word) for word in tokens]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-P3GVhyr0Da",
        "outputId": "72081826-7015-4566-a17b-3f5ae6c3733f"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dil', 'mera', 'tod', 'student', 'are', 'studi', 'and', 'learn', 'machin', 'learn', 'techniqu', 'daili']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FSMQfPf_rzm6"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program 4"
      ],
      "metadata": {
        "id": "G_Ig1beQYwpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "sentence = \"I am learning Natural Language Processing\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "filtered_stems = [\n",
        "    ps.stem(word) for word in tokens if word.lower() not in stop_words\n",
        "]\n",
        "\n",
        "print(filtered_stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeF5dRp1YMAi",
        "outputId": "0ec85f38-36c0-4a91-dae0-4aa2cb2e25fa"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['learn', 'natur', 'languag', 'process']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program 5"
      ],
      "metadata": {
        "id": "FC19OaUlYyET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"I love machine learning\",\n",
        "        \"He is studying data science\",\n",
        "        \"NLP is very interesting\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df[\"stemmed_text\"] = df[\"text\"].apply(\n",
        "    lambda x: \" \".join([ps.stem(word) for word in word_tokenize(x)])\n",
        ")\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcHWFk8BYf_s",
        "outputId": "ba8d8ff8-746c-45c1-8844-332e58c9fb78"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          text             stemmed_text\n",
            "0      I love machine learning      i love machin learn\n",
            "1  He is studying data science  he is studi data scienc\n",
            "2      NLP is very interesting     nlp is veri interest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2Ô∏è‚É£ Snowball Stemmer (Improved Porter)"
      ],
      "metadata": {
        "id": "uVbkl90gX5bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ What is Snowball Stemmer?\n",
        "\n",
        "Snowball Stemmer is an improved and more consistent version of Porter Stemmer.\n",
        "It is also known as Porter2 Stemmer.\n",
        "\n",
        "üëâ It is:\n",
        "\n",
        "More accurate than Porter\n",
        "\n",
        "Slightly more aggressive\n",
        "\n",
        "Supports multiple languages"
      ],
      "metadata": {
        "id": "cxgSL_tAtRb4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b59cbbd",
        "outputId": "faa2769d-ecce-4712-cf56-31182d004fbf"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program 1"
      ],
      "metadata": {
        "id": "EzKyqF64ZIqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "ss = SnowballStemmer(\"english\")\n",
        "ss.stem(\"studies\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QXmwOWWzWSeb",
        "outputId": "53824dd7-3720-4b5c-9bce-9ad88626ebe2"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'studi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program 2"
      ],
      "metadata": {
        "id": "uCDaan4-ZNOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "sentence = \"He was studying machines and learning NLP\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "stemmed = [stemmer.stem(word) for word in tokens]\n",
        "print(stemmed)\n"
      ],
      "metadata": {
        "id": "8lR1UEc2Y8o4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ff65809-7d7f-4b83-d405-e152346e8c51"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['he', 'was', 'studi', 'machin', 'and', 'learn', 'nlp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "sentence =\"Organizations are organizing and organized many events\"\n",
        "\n",
        "tokens=word_tokenize(sentence)\n",
        "\n",
        "stemmed=[stemmer.stem(word) for word in tokens]\n",
        "print(stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOgvWZdBtWzZ",
        "outputId": "9ba8711a-5c0c-42a1-e68b-b78252071467"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['organ', 'are', 'organ', 'and', 'organ', 'mani', 'event']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program 3"
      ],
      "metadata": {
        "id": "goMv_w9RZOXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "sentence = \"Natural Language Processing is very interesting\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "processed = [\n",
        "    stemmer.stem(word) for word in tokens\n",
        "    if word.lower() not in stop_words\n",
        "]\n",
        "\n",
        "print(processed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFHppRJjZCdP",
        "outputId": "833f7b58-a36f-4655-e23d-6380d0181ebd"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natur', 'languag', 'process', 'interest']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program 4"
      ],
      "metadata": {
        "id": "1ElFcMELZPvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"text\": [\n",
        "        \"Students are studying NLP\",\n",
        "        \"Machines are learning patterns\",\n",
        "        \"Snowball stemmer improves stemming\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "df[\"snowball_stemmed\"] = df[\"text\"].apply(\n",
        "    lambda x: \" \".join([stemmer.stem(word) for word in word_tokenize(x)])\n",
        ")\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSi6v1-rZFKm",
        "outputId": "42443dfb-6c3d-49be-e2c8-1b6219b79eb4"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 text             snowball_stemmed\n",
            "0           Students are studying NLP        student are studi nlp\n",
            "1      Machines are learning patterns     machin are learn pattern\n",
            "2  Snowball stemmer improves stemming  snowbal stemmer improv stem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3Ô∏è‚É£ Lancaster Stemmer (Very Aggressive)"
      ],
      "metadata": {
        "id": "WWeegJczX2mI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ What is Lancaster Stemmer?\n",
        "\n",
        "Lancaster Stemmer is a rule-based stemming algorithm that is much more aggressive than:\n",
        "\n",
        "Porter Stemmer\n",
        "\n",
        "Snowball Stemmer\n",
        "\n",
        "üëâ It cuts words very heavily, often producing very short stems that may lose meaning."
      ],
      "metadata": {
        "id": "5nErXtQptJuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program"
      ],
      "metadata": {
        "id": "q-vVXSmDpqe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "ls = LancasterStemmer()\n",
        "ls.stem(\"maximum\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tNEapQMmXrOq",
        "outputId": "b3a4ccc1-aec5-4ae5-8f10-2b99ff956239"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'maxim'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program"
      ],
      "metadata": {
        "id": "ioA_Gzg4ptor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "ls = LancasterStemmer()\n",
        "\n",
        "print(ls.stem(\"running\"))\n",
        "print(ls.stem(\"maximum\"))\n",
        "print(ls.stem(\"organization\"))\n"
      ],
      "metadata": {
        "id": "l1obSpwEXyAJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b63d171-ae1b-430e-b421-a6d496f7d354"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "maxim\n",
            "org\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All in one words Stemmings"
      ],
      "metadata": {
        "id": "Z7D0ehyY6j7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "ls = LancasterStemmer()\n",
        "ps=PorterStemmer()\n",
        "ss=SnowballStemmer(\"english\")\n",
        "\n",
        "words = [\"running\", \"runs\", \"runner\", \"studies\", \"easily\"]\n",
        "stems = [ls.stem(word) for word in words]\n",
        "stems2 = [ps.stem(word) for word in words]\n",
        "stems3 = [ss.stem(word) for word in words]\n",
        "\n",
        "print(stems)\n",
        "print(stems2)\n",
        "print(stems3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcUHYqEo5fDq",
        "outputId": "364732e5-c17d-4410-bb63-945807aa4b4c"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'run', 'run', 'study', 'easy']\n",
            "['run', 'run', 'runner', 'studi', 'easili']\n",
            "['run', 'run', 'runner', 'studi', 'easili']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPevhT1W5epg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison between PorterStemmer, SnowballStemer,LancasterStemer"
      ],
      "metadata": {
        "id": "a51ZIst1pCTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "words = [\"studies\", \"running\", \"maximum\", \"organization\"]\n",
        "\n",
        "for word in words:\n",
        "    print(word,\n",
        "          \"| Porter:\", porter.stem(word),\n",
        "          \"| Snowball:\", snowball.stem(word),\n",
        "          \"| Lancaster:\", lancaster.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvyOz6-Ho2Z0",
        "outputId": "84c9abbd-9722-4514-896a-1b0bbfa93a9f"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "studies | Porter: studi | Snowball: studi | Lancaster: study\n",
            "running | Porter: run | Snowball: run | Lancaster: run\n",
            "maximum | Porter: maximum | Snowball: maximum | Lancaster: maxim\n",
            "organization | Porter: organ | Snowball: organ | Lancaster: org\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program"
      ],
      "metadata": {
        "id": "qwmxJsgspvn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ls = LancasterStemmer()\n",
        "\n",
        "sentence = \"Students are studying organizations carefully\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "stems = [ls.stem(word) for word in tokens]\n",
        "print(stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UDBitCAo9Sm",
        "outputId": "b19263a6-6b52-469d-9dd1-265f65996667"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['stud', 'ar', 'study', 'org', 'car']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program"
      ],
      "metadata": {
        "id": "xKMfG3fTpxTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All in one Sentence Stemming"
      ],
      "metadata": {
        "id": "8VqscmRf6cf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ls = LancasterStemmer()\n",
        "ps=PorterStemmer()\n",
        "ss=SnowballStemmer(\"english\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "sentence = \"Natural Language Processing is extremely powerful\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "processed = [\n",
        "    ls.stem(word) for word in tokens\n",
        "\n",
        "    if word.lower() not in stop_words\n",
        "]\n",
        "processed2 = [\n",
        "    ps.stem(word) for word in tokens\n",
        "\n",
        "    if word.lower() not in stop_words\n",
        "]\n",
        "processed3 = [\n",
        "    ss.stem(word) for word in tokens\n",
        "\n",
        "    if word.lower() not in stop_words\n",
        "]\n",
        "print(processed)\n",
        "print(processed2)\n",
        "print(processed3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smzsvGknpO2p",
        "outputId": "765f8420-1014-4476-b5f0-4bb597e48d29"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nat', 'langu', 'process', 'extrem', 'pow']\n",
            "['natur', 'languag', 'process', 'extrem', 'power']\n",
            "['natur', 'languag', 'process', 'extrem', 'power']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program"
      ],
      "metadata": {
        "id": "j9GK4lIZpy_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ls = LancasterStemmer()\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"text\": [\n",
        "        \"Students are studying data science\",\n",
        "        \"Organizations are growing rapidly\",\n",
        "        \"Lancaster stemmer is aggressive\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "df[\"lancaster_stemmed\"] = df[\"text\"].apply(\n",
        "    lambda x: \" \".join([ls.stem(word) for word in word_tokenize(x)])\n",
        ")\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iavB-MAMpgS2",
        "outputId": "017fa800-6672-4cd0-8a1a-b78d1f285b85"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 text        lancaster_stemmed\n",
            "0  Students are studying data science    stud ar study dat sci\n",
            "1   Organizations are growing rapidly        org ar grow rapid\n",
            "2     Lancaster stemmer is aggressive  lancast stem is aggress\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dTKkfpn0ppJw"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a711886d"
      },
      "source": [
        "### Lemmatization with NLTK\n",
        "\n",
        "**Lemmatization** is a more sophisticated and linguistically informed process than stemming. Its goal is to reduce words to their base or dictionary form, known as a **lemma**. Unlike stemming, lemmatization guarantees that the resulting word is a valid word in the language.\n",
        "\n",
        "**Key Characteristics of Lemmatization:**\n",
        "\n",
        "1.  **Linguistic Knowledge:** It uses lexical knowledge bases (like WordNet in NLTK) and morphological analysis to determine the root form of a word. This means it considers the word's meaning and part of speech.\n",
        "2.  **Produces Valid Words:** The output of lemmatization is always a proper dictionary word, making the results more interpretable and useful for tasks requiring high linguistic accuracy.\n",
        "3.  **Context-Dependent:** It can take into account the part of speech (POS) of a word to provide a more accurate lemma. For example, 'leaves' can be the plural of 'leaf' (noun) or the third-person singular of 'leave' (verb). Lemmatization can distinguish these based on context.\n",
        "4.  **Slower than Stemming:** Due to its reliance on dictionaries and more complex algorithms, lemmatization is generally slower and more computationally intensive than stemming.\n",
        "\n",
        "**Comparison with Stemming:**\n",
        "\n",
        "| Feature           | Stemming                                | Lemmatization                                      |\n",
        "| :---------------- | :-------------------------------------- | :------------------------------------------------- |\n",
        "| **Output**        | Often a truncated string (not always a valid word) | Always a valid dictionary word (lemma)             |\n",
        "| **Method**        | Heuristic rules, suffix/prefix removal  | Dictionary-based, morphological analysis           |\n",
        "| **Speed**         | Faster                                  | Slower                                             |\n",
        "| **Accuracy**      | Less accurate, can over/under-stem      | More accurate, linguistically sound                |\n",
        "| **Context**       | Word-independent                        | Can be context-dependent (with POS tagging)        |\n",
        "| **Use Case**      | Information retrieval, quick analysis   | Text classification, sentiment analysis, machine translation, chatbots |\n",
        "\n",
        "**NLTK's `WordNetLemmatizer`**\n",
        "\n",
        "NLTK provides the `WordNetLemmatizer` for performing lemmatization. It uses the WordNet corpus (a large lexical database of English) to look up lemmas. For best results, it's often used in conjunction with part-of-speech (POS) tagging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "989c92a9",
        "outputId": "561be4b2-6107-4c27-f962-12b7d7ac52ee"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4') # Required for some WordNet functionalities"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb35c183"
      },
      "source": [
        "### Program 1: Basic Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9001b4de",
        "outputId": "416bd703-bebe-47d2-e7d5-002aaa54104d"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words_to_lemmatize = ['running', 'runs', 'ran', 'better', 'best', 'cats', 'geese']\n",
        "\n",
        "print(\"Original words and their lemmas (without POS tag):\")\n",
        "for word in words_to_lemmatize:\n",
        "    print(f\"{word}: {lemmatizer.lemmatize(word)}\")"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words and their lemmas (without POS tag):\n",
            "running: running\n",
            "runs: run\n",
            "ran: ran\n",
            "better: better\n",
            "best: best\n",
            "cats: cat\n",
            "geese: goose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf6c6499"
      },
      "source": [
        "### Program 2: Lemmatization with Part-of-Speech (POS) Tagging\n",
        "\n",
        "Lemmatization can be significantly more accurate when the Part-of-Speech (POS) of the word is provided. The `lemmatize` method accepts a `pos` argument ('n' for noun, 'v' for verb, 'a' for adjective, 'r' for adverb). If no POS is specified, it defaults to 'n' (noun).\n",
        "\n",
        "To use POS tagging effectively, we often need to first tag the words in a sentence and then convert NLTK's POS tags to WordNet's format."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatize words"
      ],
      "metadata": {
        "id": "Xe3EQv8M9wq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKzUOzGD9kVy",
        "outputId": "ab305812-2fc5-4515-e17e-878eb398ba36"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatize a Sentence"
      ],
      "metadata": {
        "id": "PrRJM_hf9vj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"Students are studying and studied machine learning\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "lemmas = [lemmatizer.lemmatize(word, pos=\"v\") for word in tokens]\n",
        "print(lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1HqRcoE95qc",
        "outputId": "471d0491-7587-42be-fcd3-8351d0b55c83"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Students', 'be', 'study', 'and', 'study', 'machine', 'learn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc70dee2"
      },
      "source": [
        "### Detailed Code Explanation\n",
        "\n",
        "*   **`from nltk.stem import WordNetLemmatizer`**: This line imports the `WordNetLemmatizer` class from the `nltk.stem` module. This is the primary tool we'll use for lemmatization.\n",
        "\n",
        "*   **`from nltk.tokenize import word_tokenize`**: This line imports the `word_tokenize` function from the `nltk.tokenize` module. This function is used to break down a sentence into individual words.\n",
        "\n",
        "*   **`lemmatizer = WordNetLemmatizer()`**: Here, an instance of the `WordNetLemmatizer` is created. This object will be used to perform the lemmatization operations.\n",
        "\n",
        "*   **`sentence = \"Students are studying and studied machine learning\"`**: This defines the input string (a sentence) that we want to lemmatize.\n",
        "\n",
        "*   **`tokens = word_tokenize(sentence)`**: This line uses the `word_tokenize` function to split the `sentence` into a list of individual words, or 'tokens'. For example, \"Students are studying...\" would become `['Students', 'are', 'studying', 'and', 'studied', 'machine', 'learning']`.\n",
        "\n",
        "*   **`lemmas = [lemmatizer.lemmatize(word, pos=\"v\") for word in tokens]`**: This is the core of the lemmatization process. It's a list comprehension that iterates through each `word` in the `tokens` list. For each word:\n",
        "    *   `lemmatizer.lemmatize(word, pos=\"v\")` is called. The `pos=\"v\"` argument is crucial here; it tells the lemmatizer to treat the word as a **verb**. This allows it to correctly identify the base form for verbs like \"studying\" (study) and \"studied\" (study), and \"are\" (be).\n",
        "    *   The resulting lemma is added to the `lemmas` list.\n",
        "\n",
        "*   **`print(lemmas)`**: Finally, this line prints the list of lemmatized words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da830927"
      },
      "source": [
        "### Explanation of Lemmatization Output\n",
        "\n",
        "The code executed successfully, producing the output: `['Students', 'be', 'study', 'and', 'study', 'machine', 'learn']`.\n",
        "\n",
        "Here's a breakdown of what happened:\n",
        "\n",
        "*   **`'Students'`**: This word remained unchanged. Even though we specified `pos='v'` (verb), 'Students' is primarily a noun, and the lemmatizer wouldn't reduce it further when treated as a verb.\n",
        "*   **`'be'`**: The word 'are' was correctly lemmatized to its base verb form, 'be'.\n",
        "*   **`'study'`**: Both 'studying' and 'studied' were successfully reduced to their base verb form, 'study', due to the `pos='v'` argument.\n",
        "*   **`'machine'`**: This word remained 'machine'. While it can be a verb, in this context, its base form is itself when treated as a verb.\n",
        "*   **`'learn'`**: The word 'learning' (when functioning as a verb here, likely a gerund or present participle) was lemmatized to its base verb form, 'learn'.\n",
        "\n",
        "This output clearly demonstrates how specifying the Part-of-Speech (`pos='v'`) guides the `WordNetLemmatizer` to find the correct base form for words when they function as verbs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c6b18fb",
        "outputId": "707231c5-1af8-4ad5-b841-100028f9e268"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"Students are studying and studied machine learning\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "lemmas = [lemmatizer.lemmatize(word, pos=\"v\") for word in tokens]\n",
        "print(lemmas)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Students', 'be', 'study', 'and', 'study', 'machine', 'learn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stopwords Removal + Lemmatization"
      ],
      "metadata": {
        "id": "fRF0IOav-E_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "sentence = \"Students are studying natural language processing\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "processed = [\n",
        "    lemmatizer.lemmatize(word, pos=\"v\")\n",
        "    for word in tokens if word.lower() not in stop_words\n",
        "]\n",
        "\n",
        "print(processed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6RJ8kgD-IFi",
        "outputId": "410a1162-7c47-4587-8eca-d081541f036d"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Students', 'study', 'natural', 'language', 'process']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05207778",
        "outputId": "0c2be497-b56a-4660-bb6f-7587b40c5f50"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e749ff1c"
      },
      "source": [
        "### Example: Removing Stopwords with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9de1b3d",
        "outputId": "d777555e-022c-4707-84c5-f8d3b1391bdd"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"This is an example sentence, demonstrating the removal of common stopwords in natural language processing.\"\n",
        "\n",
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize the text\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "# Filter out stopwords\n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words and w.isalnum()]\n",
        "\n",
        "print(f\"Original sentence: {text}\")\n",
        "print(f\"Filtered sentence (without stopwords): {filtered_sentence}\")"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: This is an example sentence, demonstrating the removal of common stopwords in natural language processing.\n",
            "Filtered sentence (without stopwords): ['example', 'sentence', 'demonstrating', 'removal', 'common', 'stopwords', 'natural', 'language', 'processing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d83b62e",
        "outputId": "fdf781b2-edef-4f19-f401-b5ff5ab67f69"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN # Default to noun if POS not found\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "sentence = \"The quick brown foxes are running quickly through the beautiful forest, connecting with nature.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "lemmatized_words = []\n",
        "for word, tag in pos_tag(tokens):\n",
        "    wordnet_pos = get_wordnet_pos(tag)\n",
        "    lemmatized_words.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
        "\n",
        "print(f\"Original sentence: {sentence}\")\n",
        "print(f\"Lemmatized words (with POS): {lemmatized_words}\")\n",
        "\n",
        "sentence2 = \"He is driving his car, and he drove to the store.\"\n",
        "tokens2 = word_tokenize(sentence2)\n",
        "lemmatized_words2 = []\n",
        "for word, tag in pos_tag(tokens2):\n",
        "    wordnet_pos = get_wordnet_pos(tag)\n",
        "    lemmatized_words2.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
        "\n",
        "print(f\"\\nOriginal sentence: {sentence2}\")\n",
        "print(f\"Lemmatized words (with POS): {lemmatized_words2}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: The quick brown foxes are running quickly through the beautiful forest, connecting with nature.\n",
            "Lemmatized words (with POS): ['The', 'quick', 'brown', 'fox', 'be', 'run', 'quickly', 'through', 'the', 'beautiful', 'forest', ',', 'connect', 'with', 'nature', '.']\n",
            "\n",
            "Original sentence: He is driving his car, and he drove to the store.\n",
            "Lemmatized words (with POS): ['He', 'be', 'drive', 'his', 'car', ',', 'and', 'he', 'drive', 'to', 'the', 'store', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dee8549"
      },
      "source": [
        "### Program 3: Lemmatization on a Pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5433ac1d",
        "outputId": "ca0e5783-f7db-441e-e7e1-976b85f9c5b9"
      },
      "source": [
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "df_lem = pd.DataFrame({\n",
        "    \"text\": [\n",
        "        \"Cats are running very fast on the green grass.\",\n",
        "        \"The women were singing and enjoying the beautiful scenery.\",\n",
        "        \"He has a better understanding of the data.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_words = []\n",
        "    for word, tag in pos_tag(tokens):\n",
        "        wordnet_pos = get_wordnet_pos(tag)\n",
        "        lemmatized_words.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
        "    return \" \".join(lemmatized_words)\n",
        "\n",
        "df_lem[\"lemmatized_text\"] = df_lem[\"text\"].apply(lemmatize_text)\n",
        "\n",
        "print(df_lem)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0     Cats are running very fast on the green grass.   \n",
            "1  The women were singing and enjoying the beauti...   \n",
            "2         He has a better understanding of the data.   \n",
            "\n",
            "                                     lemmatized_text  \n",
            "0         Cats be run very fast on the green grass .  \n",
            "1  The woman be sing and enjoy the beautiful scen...  \n",
            "2         He have a good understanding of the data .  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"text\": [\n",
        "        \"Students are studying NLP\",\n",
        "        \"Researchers analyzed datasets\",\n",
        "        \"Machines are learning patterns\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "df[\"lemmatized_text\"] = df[\"text\"].apply(\n",
        "    lambda x: \" \".join([lemmatizer.lemmatize(word, pos=\"v\") for word in word_tokenize(x)])\n",
        ")\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5NjHQrL_mtp",
        "outputId": "fc017a07-d151-41b7-ba66-a99381d9fdce"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             text               lemmatized_text\n",
            "0       Students are studying NLP         Students be study NLP\n",
            "1   Researchers analyzed datasets  Researchers analyze datasets\n",
            "2  Machines are learning patterns     Machines be learn pattern\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AoFgtjMZ_oLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00131c9c"
      },
      "source": [
        "### Stopwords\n",
        "\n",
        "**Stopwords** are common words that appear frequently in any language but often carry little or no significant meaning for text analysis tasks. These words are typically filtered out from text before processing, as they can add noise and unnecessary computational overhead without contributing much to the overall understanding or differentiation of documents.\n",
        "\n",
        "**Why Remove Stopwords?**\n",
        "\n",
        "1.  **Reduce Noise:** Stopwords are very common (e.g., 'the', 'is', 'a', 'an', 'in') and don't usually help in distinguishing between different documents or topics. Removing them helps focus on more meaningful terms.\n",
        "2.  **Reduce Dimensionality:** By eliminating frequent, non-informative words, the size of the vocabulary (feature space) is significantly reduced. This leads to faster processing and less memory usage, which is especially beneficial in large datasets.\n",
        "3.  **Improve Performance:** In many NLP tasks like text classification, sentiment analysis, and information retrieval, stopwords can skew results or reduce the effectiveness of algorithms. Removing them often improves the accuracy and efficiency of these tasks.\n",
        "4.  **Focus on Key Terms:** It allows analysis to concentrate on the more important, content-bearing words, providing a clearer picture of the text's subject matter.\n",
        "\n",
        "**Common Stopwords:**\n",
        "\n",
        "Examples of English stopwords include: 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'.\n",
        "\n",
        "**NLTK's Stopwords Corpus:**\n",
        "\n",
        "NLTK provides a pre-defined list of stopwords for various languages in its `stopwords` corpus. You can easily access and use this list to filter stopwords from your text."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdD1USa_Bp1a",
        "outputId": "8f269035-dcc9-444a-d2a6-6cd2cafe7ad3"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ List Available Languages"
      ],
      "metadata": {
        "id": "M7FJQ4pPB2nU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "print(stopwords.fileids())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8WapF1UBqXL",
        "outputId": "d09a10e1-2c36-43e4-f84a-385f503e3ba6"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['albanian', 'arabic', 'azerbaijani', 'basque', 'belarusian', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'tamil', 'turkish', 'uzbek']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Get English Stopwords"
      ],
      "metadata": {
        "id": "KHXQvcIlB5MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(len(stop_words))\n",
        "print(list(stop_words)[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fINXN0HTBvPw",
        "outputId": "bba722bc-b55d-4f52-bd8c-25156d68387f"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "198\n",
            "[\"i'll\", 'than', 'the', 'off', 'as', 'where', 'o', \"should've\", 'for', 'again']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt=\"This is not a good time to talk.\"\n",
        "txt=word_tokenize(txt)\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dLzrCeZCmp7",
        "outputId": "93ce4c36-9449-4e7a-9a08-a88e096be26f"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'not', 'a', 'good', 'time', 'to', 'talk', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in txt:\n",
        "    if word.lower() not in stop_words:\n",
        "        print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vH5pywuCy3n",
        "outputId": "d7e98d5d-a90d-4864-8b90-c012e606c165"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good\n",
            "time\n",
            "talk\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'This'.lower()=='this'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAlXlnciC4u5",
        "outputId": "0d0b3711-337b-4e86-d82f-60f12b6bf441"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt2=\"This is not a good time to talk.Can we Do it now ?\"\n",
        "txt2=word_tokenize(txt2)\n",
        "print(txt2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQcp0RMyDLFE",
        "outputId": "457fa767-74c1-4fc3-8d11-bd011279bb87"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'not', 'a', 'good', 'time', 'to', 'talk.Can', 'we', 'Do', 'it', 'now', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in txt2:\n",
        "    if word.lower() not in stop_words:\n",
        "        print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKNYh8ZsDWON",
        "outputId": "01274bca-f693-48a4-fb16-9857e82902c5"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good\n",
            "time\n",
            "talk.Can\n",
            "?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üõë Program 1: Remove Stopwords from a Sentence"
      ],
      "metadata": {
        "id": "xxLSMHSSB8HC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "sentence = \"This is a simple example to understand stopwords\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaNwaoUNByRr",
        "outputId": "0dbe46ed-67a8-4fa5-eef9-502c524079b7"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['simple', 'example', 'understand', 'stopwords']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üõë Program 2: Stopwords + Stemming"
      ],
      "metadata": {
        "id": "3qWLTbgaCI_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "sentence = \"Students are studying and learning natural language processing\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "processed = [\n",
        "    ps.stem(word) for word in tokens if word.lower() not in stop_words\n",
        "]\n",
        "\n",
        "print(processed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3gweJM5CECK",
        "outputId": "ab43d59c-b3a0-4651-9837-d97ea83f700e"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['student', 'studi', 'learn', 'natur', 'languag', 'process']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üõë Program 3: Stopwords + Lemmatization"
      ],
      "metadata": {
        "id": "DXKthu4HCO7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "sentence = \"Researchers are analyzing large datasets in NLP\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "processed = [\n",
        "    lemmatizer.lemmatize(word, pos=\"v\")\n",
        "    for word in tokens if word.lower() not in stop_words\n",
        "]\n",
        "\n",
        "print(processed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnxRXPy4CMpu",
        "outputId": "7d220896-711c-4ee2-9758-dd21ba88198f"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Researchers', 'analyze', 'large', 'datasets', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üõë Program 4: Stopwords on Pandas DataFrame"
      ],
      "metadata": {
        "id": "_4j9WtQoChrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"text\": [\n",
        "        \"This is an NLP example\",\n",
        "        \"Stopwords removal improves models\",\n",
        "        \"Not all stopwords should be removed\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(\n",
        "    lambda x: \" \".join(\n",
        "        [word for word in word_tokenize(x) if word.lower() not in stop_words]\n",
        "    )\n",
        ")\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch1QRg1ICREV",
        "outputId": "5e56541d-8a25-43f8-c0dc-0077c9a7eb82"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  text                         clean_text\n",
            "0               This is an NLP example                        NLP example\n",
            "1    Stopwords removal improves models  Stopwords removal improves models\n",
            "2  Not all stopwords should be removed                  stopwords removed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea5baf97"
      },
      "source": [
        "### Corpus (in NLP)\n",
        "\n",
        "A **corpus** (plural: **corpora**) is a large and structured set of texts. In the field of Natural Language Processing (NLP) and computational linguistics, a corpus serves as a fundamental resource for studying language, developing models, and evaluating algorithms.\n",
        "\n",
        "Think of it as a carefully curated collection of written or spoken language data, often gathered for specific research purposes.\n",
        "\n",
        "**Key Characteristics of a Corpus:**\n",
        "\n",
        "1.  **Size:** Corpora are typically large, containing millions or even billions of words. This ensures that the data is representative of the language being studied and allows for statistical analysis of linguistic patterns.\n",
        "2.  **Representativeness:** A good corpus should be representative of the language or the specific domain it aims to cover. For example, a corpus designed for general English might include texts from various genres (news, fiction, academic papers, conversations), while a specialized medical corpus would focus on medical literature.\n",
        "3.  **Machine-Readable:** Corpora are stored in electronic formats that can be processed and analyzed by computers. This is crucial for applying NLP techniques.\n",
        "4.  **Annotation (Optional but Common):** Many corpora are enriched with linguistic annotations, which means they have extra information added to them. Common types of annotation include:\n",
        "    *   **Part-of-Speech (POS) Tagging:** Marking each word with its grammatical category (e.g., noun, verb, adjective).\n",
        "    *   **Lemmatization/Stemming:** Providing the base form of words.\n",
        "    *   **Syntactic Parsing:** Analyzing the grammatical structure of sentences.\n",
        "    *   **Named Entity Recognition (NER):** Identifying proper nouns like person names, organizations, and locations.\n",
        "    *   **Semantic Annotation:** Marking word senses or thematic roles.\n",
        "\n",
        "**Types of Corpora:**\n",
        "\n",
        "*   **General Corpora:** Aim to represent a broad range of language use (e.g., Brown Corpus, British National Corpus).\n",
        "*   **Specialized Corpora:** Focus on specific domains, genres, or time periods (e.g., medical texts, legal documents, historical literature).\n",
        "*   **Monolingual Corpora:** Contain texts in a single language.\n",
        "*   **Multilingual/Parallel Corpora:** Contain texts in two or more languages, often with sentences or paragraphs aligned for translation tasks.\n",
        "*   **Annotated Corpora:** Corpora with added linguistic information.\n",
        "\n",
        "**Why are Corpora Important in NLP?**\n",
        "\n",
        "1.  **Training Data:** They are essential for training machine learning models used in NLP tasks such as text classification, sentiment analysis, machine translation, speech recognition, and more.\n",
        "2.  **Linguistic Research:** Linguists use corpora to study language patterns, grammar, vocabulary, and semantic change.\n",
        "3.  **Algorithm Development:** Developers test and refine NLP algorithms on corpora to ensure they perform accurately and efficiently.\n",
        "4.  **Lexicography:** Creating dictionaries and lexical resources relies heavily on corpora to identify common words, phrases, and their uses.\n",
        "\n",
        "**NLTK and Corpora:**\n",
        "\n",
        "NLTK (Natural Language Toolkit) provides easy access to a variety of corpora and lexical resources. When you use NLTK functions like `nltk.download('wordnet')` or `nltk.download('stopwords')`, you are downloading components of different corpora that are bundled with NLTK. These pre-packaged corpora allow users to quickly start experimenting with NLP without having to build their own datasets from scratch."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fiRHFo1XEvjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "654fedbd",
        "outputId": "7d5219ac-f7a7-4640-a6db-54976183d8ec"
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c88624e",
        "outputId": "4828e25f-1cd2-4890-cda5-3603cc95bdb6"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "\n",
        "print(brown.fileids()[:5])\n",
        "print(brown.words()[:10])"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ca01', 'ca02', 'ca03', 'ca04', 'ca05']\n",
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "128978a7",
        "outputId": "81c3a992-7210-4d04-87df-58d683dab5f8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46d55266",
        "outputId": "04002792-3e03-45cf-9db2-966e783930cb"
      },
      "source": [
        "from nltk.corpus import gutenberg\n",
        "\n",
        "print(gutenberg.fileids())\n",
        "\n",
        "words = gutenberg.words('austen-emma.txt')\n",
        "print(words[:20])"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=\"Gutenberg's invention of the mechanical movable type printing press around 1440 sparked a print revolution, transforming hand-copied manuscripts into mass-produced, cheaper books like the Gutenberg Bible, fostering literacy, spreading knowledge, and enabling the Reformation and Renaissance by creating a new reading public and culture of debate. This technology allowed for rapid production, making books accessible beyond the elite and fundamentally shifting societies from oral traditions to print culture\""
      ],
      "metadata": {
        "id": "joFnDiVeE72k"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "WidCpxdkFjB0",
        "outputId": "f97e506b-c8b8-4e25-d6fe-1d1d61084717"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Gutenberg's invention of the mechanical movable type printing press around 1440 sparked a print revolution, transforming hand-copied manuscripts into mass-produced, cheaper books like the Gutenberg Bible, fostering literacy, spreading knowledge, and enabling the Reformation and Renaissance by creating a new reading public and culture of debate. This technology allowed for rapid production, making books accessible beyond the elite and fundamentally shifting societies from oral traditions to print culture\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "9LTTtQjQFrz6"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9pzlnddcGCa_",
        "outputId": "33097bc0-c580-4249-fc70-436f35878ecf"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Gutenberg',\n",
              " \"'s\",\n",
              " 'invention',\n",
              " 'of',\n",
              " 'the',\n",
              " 'mechanical',\n",
              " 'movable',\n",
              " 'type',\n",
              " 'printing',\n",
              " 'press',\n",
              " 'around',\n",
              " '1440',\n",
              " 'sparked',\n",
              " 'a',\n",
              " 'print',\n",
              " 'revolution',\n",
              " ',',\n",
              " 'transforming',\n",
              " 'hand-copied',\n",
              " 'manuscripts',\n",
              " 'into',\n",
              " 'mass-produced',\n",
              " ',',\n",
              " 'cheaper',\n",
              " 'books',\n",
              " 'like',\n",
              " 'the',\n",
              " 'Gutenberg',\n",
              " 'Bible',\n",
              " ',',\n",
              " 'fostering',\n",
              " 'literacy',\n",
              " ',',\n",
              " 'spreading',\n",
              " 'knowledge',\n",
              " ',',\n",
              " 'and',\n",
              " 'enabling',\n",
              " 'the',\n",
              " 'Reformation',\n",
              " 'and',\n",
              " 'Renaissance',\n",
              " 'by',\n",
              " 'creating',\n",
              " 'a',\n",
              " 'new',\n",
              " 'reading',\n",
              " 'public',\n",
              " 'and',\n",
              " 'culture',\n",
              " 'of',\n",
              " 'debate',\n",
              " '.',\n",
              " 'This',\n",
              " 'technology',\n",
              " 'allowed',\n",
              " 'for',\n",
              " 'rapid',\n",
              " 'production',\n",
              " ',',\n",
              " 'making',\n",
              " 'books',\n",
              " 'accessible',\n",
              " 'beyond',\n",
              " 'the',\n",
              " 'elite',\n",
              " 'and',\n",
              " 'fundamentally',\n",
              " 'shifting',\n",
              " 'societies',\n",
              " 'from',\n",
              " 'oral',\n",
              " 'traditions',\n",
              " 'to',\n",
              " 'print',\n",
              " 'culture']"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    if word.lower() not in stop_words:\n",
        "        print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ymmcUCMWGDIV",
        "outputId": "207d336c-cc87-4bb8-cc2d-115b4d40f497"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gutenberg\n",
            "'s\n",
            "invention\n",
            "mechanical\n",
            "movable\n",
            "type\n",
            "printing\n",
            "press\n",
            "around\n",
            "1440\n",
            "sparked\n",
            "print\n",
            "revolution\n",
            ",\n",
            "transforming\n",
            "hand-copied\n",
            "manuscripts\n",
            "mass-produced\n",
            ",\n",
            "cheaper\n",
            "books\n",
            "like\n",
            "Gutenberg\n",
            "Bible\n",
            ",\n",
            "fostering\n",
            "literacy\n",
            ",\n",
            "spreading\n",
            "knowledge\n",
            ",\n",
            "enabling\n",
            "Reformation\n",
            "Renaissance\n",
            "creating\n",
            "new\n",
            "reading\n",
            "public\n",
            "culture\n",
            "debate\n",
            ".\n",
            "technology\n",
            "allowed\n",
            "rapid\n",
            "production\n",
            ",\n",
            "making\n",
            "books\n",
            "accessible\n",
            "beyond\n",
            "elite\n",
            "fundamentally\n",
            "shifting\n",
            "societies\n",
            "oral\n",
            "traditions\n",
            "print\n",
            "culture\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_tokenize(corpus):\n",
        "  if(word.lower() not in stopwords.words('english')):\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ya1LUABbGw90",
        "outputId": "acc1a621-a4a7-4ae0-f2d2-1aca9816e7cf"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gutenberg\n",
            "'s\n",
            "invention\n",
            "mechanical\n",
            "movable\n",
            "type\n",
            "printing\n",
            "press\n",
            "around\n",
            "1440\n",
            "sparked\n",
            "print\n",
            "revolution\n",
            ",\n",
            "transforming\n",
            "hand-copied\n",
            "manuscripts\n",
            "mass-produced\n",
            ",\n",
            "cheaper\n",
            "books\n",
            "like\n",
            "Gutenberg\n",
            "Bible\n",
            ",\n",
            "fostering\n",
            "literacy\n",
            ",\n",
            "spreading\n",
            "knowledge\n",
            ",\n",
            "enabling\n",
            "Reformation\n",
            "Renaissance\n",
            "creating\n",
            "new\n",
            "reading\n",
            "public\n",
            "culture\n",
            "debate\n",
            ".\n",
            "technology\n",
            "allowed\n",
            "rapid\n",
            "production\n",
            ",\n",
            "making\n",
            "books\n",
            "accessible\n",
            "beyond\n",
            "elite\n",
            "fundamentally\n",
            "shifting\n",
            "societies\n",
            "oral\n",
            "traditions\n",
            "print\n",
            "culture\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_tokenize(corpus):\n",
        "  if(word.lower() not in stopwords.words('english')) and (len(word)>=2):\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-D0XYeBJHygY",
        "outputId": "27097038-8ae9-4b52-f7a5-b019ac6109a8"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gutenberg\n",
            "'s\n",
            "invention\n",
            "mechanical\n",
            "movable\n",
            "type\n",
            "printing\n",
            "press\n",
            "around\n",
            "1440\n",
            "sparked\n",
            "print\n",
            "revolution\n",
            "transforming\n",
            "hand-copied\n",
            "manuscripts\n",
            "mass-produced\n",
            "cheaper\n",
            "books\n",
            "like\n",
            "Gutenberg\n",
            "Bible\n",
            "fostering\n",
            "literacy\n",
            "spreading\n",
            "knowledge\n",
            "enabling\n",
            "Reformation\n",
            "Renaissance\n",
            "creating\n",
            "new\n",
            "reading\n",
            "public\n",
            "culture\n",
            "debate\n",
            "technology\n",
            "allowed\n",
            "rapid\n",
            "production\n",
            "making\n",
            "books\n",
            "accessible\n",
            "beyond\n",
            "elite\n",
            "fundamentally\n",
            "shifting\n",
            "societies\n",
            "oral\n",
            "traditions\n",
            "print\n",
            "culture\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=[]\n",
        "\n",
        "for word in word_tokenize(corpus):\n",
        "  if(word.lower() not in stopwords.words('english')) and (len(word)>=2):\n",
        "    words.append(word.lower())"
      ],
      "metadata": {
        "id": "Gpn8XwtMIGJ2"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this repeated words are available"
      ],
      "metadata": {
        "id": "AOCrZGVFIw5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41MZcd6bIo5W",
        "outputId": "351d9bb9-6252-4383-fcba-c46d63fb081f"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "adceDRp-IX5A",
        "outputId": "36dd299b-2222-40fa-ed26-a636cd44ebeb"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gutenberg',\n",
              " \"'s\",\n",
              " 'invention',\n",
              " 'mechanical',\n",
              " 'movable',\n",
              " 'type',\n",
              " 'printing',\n",
              " 'press',\n",
              " 'around',\n",
              " '1440',\n",
              " 'sparked',\n",
              " 'print',\n",
              " 'revolution',\n",
              " 'transforming',\n",
              " 'hand-copied',\n",
              " 'manuscripts',\n",
              " 'mass-produced',\n",
              " 'cheaper',\n",
              " 'books',\n",
              " 'like',\n",
              " 'gutenberg',\n",
              " 'bible',\n",
              " 'fostering',\n",
              " 'literacy',\n",
              " 'spreading',\n",
              " 'knowledge',\n",
              " 'enabling',\n",
              " 'reformation',\n",
              " 'renaissance',\n",
              " 'creating',\n",
              " 'new',\n",
              " 'reading',\n",
              " 'public',\n",
              " 'culture',\n",
              " 'debate',\n",
              " 'technology',\n",
              " 'allowed',\n",
              " 'rapid',\n",
              " 'production',\n",
              " 'making',\n",
              " 'books',\n",
              " 'accessible',\n",
              " 'beyond',\n",
              " 'elite',\n",
              " 'fundamentally',\n",
              " 'shifting',\n",
              " 'societies',\n",
              " 'oral',\n",
              " 'traditions',\n",
              " 'print',\n",
              " 'culture']"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VcMVHNnBIY-A",
        "outputId": "32e2dd22-d5d0-4028-afa9-8bca508a4897"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'s\",\n",
              " '1440',\n",
              " 'accessible',\n",
              " 'allowed',\n",
              " 'around',\n",
              " 'beyond',\n",
              " 'bible',\n",
              " 'books',\n",
              " 'cheaper',\n",
              " 'creating',\n",
              " 'culture',\n",
              " 'debate',\n",
              " 'elite',\n",
              " 'enabling',\n",
              " 'fostering',\n",
              " 'fundamentally',\n",
              " 'gutenberg',\n",
              " 'hand-copied',\n",
              " 'invention',\n",
              " 'knowledge',\n",
              " 'like',\n",
              " 'literacy',\n",
              " 'making',\n",
              " 'manuscripts',\n",
              " 'mass-produced',\n",
              " 'mechanical',\n",
              " 'movable',\n",
              " 'new',\n",
              " 'oral',\n",
              " 'press',\n",
              " 'print',\n",
              " 'printing',\n",
              " 'production',\n",
              " 'public',\n",
              " 'rapid',\n",
              " 'reading',\n",
              " 'reformation',\n",
              " 'renaissance',\n",
              " 'revolution',\n",
              " 'shifting',\n",
              " 'societies',\n",
              " 'sparked',\n",
              " 'spreading',\n",
              " 'technology',\n",
              " 'traditions',\n",
              " 'transforming',\n",
              " 'type'}"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unique words"
      ],
      "metadata": {
        "id": "TnWyE59ZIu1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1UHaWRxIiJM",
        "outputId": "9ee92664-ef4f-4d0d-b6af-f5407d27e21d"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab=list(set(words))"
      ],
      "metadata": {
        "id": "81L7zS_9ImNh"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cg55dfhI8GI",
        "outputId": "8c4b8339-aaea-412a-95f9-29c8a6d154ce"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mechanical',\n",
              " 'reading',\n",
              " 'revolution',\n",
              " 'manuscripts',\n",
              " 'elite',\n",
              " 'renaissance',\n",
              " 'public',\n",
              " 'fostering',\n",
              " 'allowed',\n",
              " 'fundamentally',\n",
              " 'knowledge',\n",
              " 'printing',\n",
              " 'cheaper',\n",
              " 'beyond',\n",
              " 'enabling',\n",
              " 'societies',\n",
              " 'debate',\n",
              " 'shifting',\n",
              " 'press',\n",
              " 'invention',\n",
              " 'transforming',\n",
              " \"'s\",\n",
              " 'gutenberg',\n",
              " 'oral',\n",
              " '1440',\n",
              " 'rapid',\n",
              " 'culture',\n",
              " 'accessible',\n",
              " 'new',\n",
              " 'technology',\n",
              " 'like',\n",
              " 'books',\n",
              " 'mass-produced',\n",
              " 'bible',\n",
              " 'movable',\n",
              " 'creating',\n",
              " 'print',\n",
              " 'traditions',\n",
              " 'production',\n",
              " 'type',\n",
              " 'around',\n",
              " 'hand-copied',\n",
              " 'literacy',\n",
              " 'reformation',\n",
              " 'sparked',\n",
              " 'making',\n",
              " 'spreading']"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oJGzFyqaI9uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49869b94"
      },
      "source": [
        "### Vocabulary (in NLP)\n",
        "\n",
        "In Natural Language Processing (NLP), **vocabulary** refers to the set of all unique words (or tokens) found in a given text corpus or dataset. It's essentially a dictionary of all the distinct words that an NLP model can recognize and process.\n",
        "\n",
        "**Key Aspects of Vocabulary:**\n",
        "\n",
        "1.  **Unique Words:** Each word in the vocabulary is unique, regardless of how many times it appears in the corpus. For example, if 'run' appears 100 times and 'running' appears 50 times, both 'run' and 'running' would be distinct entries in the vocabulary (unless processed by stemming or lemmatization).\n",
        "\n",
        "2.  **Size:** The size of a vocabulary can vary greatly depending on the size and diversity of the text corpus. Large, diverse corpora will yield larger vocabularies. This is often a critical factor in NLP, as larger vocabularies can lead to more complex models and computational challenges.\n",
        "\n",
        "3.  **Tokenization:** The process of defining what constitutes a 'word' (or token) is crucial before building a vocabulary. Different tokenization methods (e.g., word tokenization, subword tokenization) will result in different vocabularies.\n",
        "\n",
        "4.  **Normalization:** Text normalization techniques like lowercasing, stemming, and lemmatization directly impact vocabulary size and content. For example, if all words are lowercased, 'The' and 'the' become a single entry. If stemming is applied, 'running', 'runs', and 'ran' might all map to 'run', reducing vocabulary size.\n",
        "\n",
        "5.  **Out-of-Vocabulary (OOV) Words:** Words encountered during inference (when using a trained model) that were not present in the training vocabulary are called Out-Of-Vocabulary (OOV) words. Handling OOV words is a significant challenge in NLP, often addressed through techniques like subword tokenization or replacing them with an `<UNK>` (unknown) token.\n",
        "\n",
        "**Importance of Vocabulary in NLP:**\n",
        "\n",
        "*   **Feature Representation:** In many NLP tasks, words are converted into numerical representations (e.g., one-hot encodings, word embeddings). The vocabulary defines the mapping from words to these numerical indices or vectors.\n",
        "*   **Model Performance:** A well-constructed vocabulary is essential for training robust NLP models. A vocabulary that is too small might miss important linguistic nuances, while one that is too large can lead to sparsity issues and increased computational cost.\n",
        "*   **Language Understanding:** The vocabulary dictates the scope of what a language model can 'understand' or generate. It's the foundation upon which more complex linguistic structures are built."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25cd92ff",
        "outputId": "a890d924-df9d-4969-83d7-cbc86be3f2f6"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Assuming 'corpus' is already defined from previous cells\n",
        "# corpus = \"Gutenberg's invention of the mechanical movable type printing press...\"\n",
        "\n",
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize and normalize the text\n",
        "words = []\n",
        "for word in word_tokenize(corpus):\n",
        "    # Convert to lowercase, remove stopwords, and filter words shorter than 2 characters\n",
        "    if word.lower() not in stop_words and len(word) >= 2:\n",
        "        words.append(word.lower())\n",
        "\n",
        "# Create the vocabulary (set of unique words)\n",
        "vocabulary = sorted(list(set(words)))\n",
        "\n",
        "print(f\"Original text length (in words, after initial filtering): {len(words)}\")\n",
        "print(f\"Vocabulary size (unique words): {len(vocabulary)}\")\n",
        "print(\"\\nFirst 10 words in the vocabulary:\")\n",
        "print(vocabulary[:10])\n",
        "\n",
        "print(\"\\nLast 10 words in the vocabulary:\")\n",
        "print(vocabulary[-10:])"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text length (in words, after initial filtering): 51\n",
            "Vocabulary size (unique words): 47\n",
            "\n",
            "First 10 words in the vocabulary:\n",
            "[\"'s\", '1440', 'accessible', 'allowed', 'around', 'beyond', 'bible', 'books', 'cheaper', 'creating']\n",
            "\n",
            "Last 10 words in the vocabulary:\n",
            "['renaissance', 'revolution', 'shifting', 'societies', 'sparked', 'spreading', 'technology', 'traditions', 'transforming', 'type']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in sent_tokenize(corpus):\n",
        "    print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcOlgy4SJkRS",
        "outputId": "50b1407f-7385-4801-e882-668a1602838d"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gutenberg's invention of the mechanical movable type printing press around 1440 sparked a print revolution, transforming hand-copied manuscripts into mass-produced, cheaper books like the Gutenberg Bible, fostering literacy, spreading knowledge, and enabling the Reformation and Renaissance by creating a new reading public and culture of debate.\n",
            "This technology allowed for rapid production, making books accessible beyond the elite and fundamentally shifting societies from oral traditions to print culture\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ygVDtcPJnsr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}