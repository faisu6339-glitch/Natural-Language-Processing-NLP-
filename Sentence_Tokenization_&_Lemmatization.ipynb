{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaHIcfqgaHu4G3Wi5JCG8k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faisu6339-glitch/Natural-Language-Processing-NLP-/blob/main/Sentence_Tokenization_%26_Lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentence_Tokenization"
      ],
      "metadata": {
        "id": "VdLbpfWAHS4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Explain how sent_tokenize works\n",
        "The sent_tokenize function from the nltk.tokenize module is used to split a given text into a list of sentences. It works by using an unsupervised algorithm to build a model for a language (like English) that can detect sentence boundaries. This model is based on a pre-trained Punkt tokenizer, which identifies punctuation marks and capitalization patterns to determine where one sentence ends and another begins. Essentially, it looks for cues like periods, question marks, and exclamation points, often combined with the capitalization of the next word, to accurately segment the text into individual sentences."
      ],
      "metadata": {
        "id": "x8-Vh3GiHbLK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4yIKib6GNSK",
        "outputId": "01d6626b-f9dc-4013-8ea4-36febd5eb460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVv3Rls_G3SX",
        "outputId": "6d140fa0-0b02-4df4-b317-712379457f9b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text='Hello everyone.Welcome to GeekForGeeks. You are studying NLP article.'\n",
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzMxCD1-GaW9",
        "outputId": "9d228043-8f45-459e-95b3-acfb0bae2ea0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello everyone.Welcome to GeekForGeeks.', 'You are studying NLP article.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word tokenization"
      ],
      "metadata": {
        "id": "_JGE2GNsHgOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word tokenization is the process of splitting a text document into individual words or tokens. It's a fundamental step in many natural language processing (NLP) tasks. Unlike sentence tokenization, which focuses on breaking text into sentences, word tokenization breaks sentences (or a continuous stream of text) into meaningful units, typically words, punctuation, and numbers. The exact definition of a 'word' can vary depending on the specific tokenizer used and the language, but generally, it involves identifying boundaries based on spaces, punctuation, and other linguistic rules. For example, in the sentence 'Hello, world!', word tokenization might produce tokens like ['Hello', ',', 'world', '!']."
      ],
      "metadata": {
        "id": "quEjGZHIHzfO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T-zf6gLiHh8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text='Hello everyone.Welcome to GeekForGeeks. You are studying NLP article.'\n",
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4cBxn02Ho0I",
        "outputId": "1abf5870-998c-49d5-ee87-d84519b32196"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'everyone.Welcome',\n",
              " 'to',\n",
              " 'GeekForGeeks',\n",
              " '.',\n",
              " 'You',\n",
              " 'are',\n",
              " 'studying',\n",
              " 'NLP',\n",
              " 'article',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word Tokenization usng Regular Expression"
      ],
      "metadata": {
        "id": "Ia7nNTrIH31Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer=RegexpTokenizer(r'\\w+')\n",
        "text='Hello everyone.Welcome to GeekForGeeks. You are studying NLP article.'\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y0Ccgh3HvgI",
        "outputId": "7bb8e3c0-a79c-4069-8136-4a95f2c22f3f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'everyone',\n",
              " 'Welcome',\n",
              " 'to',\n",
              " 'GeekForGeeks',\n",
              " 'You',\n",
              " 'are',\n",
              " 'studying',\n",
              " 'NLP',\n",
              " 'article']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Certainly! Beyond sent_tokenize and word_tokenize, NLTK offers several other tokenization methods, each suited for different needs:\n",
        "\n",
        "TreebankWordTokenizer: This is often considered the 'standard' word tokenizer in NLTK. It implements the Penn Treebank tokenization scheme, which is widely used in NLP research. It has specific rules for handling contractions, punctuation (like separating periods from abbreviations), and hyphenated words, generally producing a cleaner, more standardized tokenization for many tasks.\n",
        "\n",
        "Example: text = \"Don't go!\" might become ['Do', 'n\\'t', 'go', '!']\n",
        "\n",
        "WhitespaceTokenizer: This is a very simple tokenizer that splits text based on whitespace characters (spaces, tabs, newlines). It doesn't handle punctuation attached to words, so it's less sophisticated than word_tokenize but can be useful for quick, basic tokenization when punctuation separation isn't critical.\n",
        "\n",
        "Example: text = \"Hello, world!\" might become ['Hello,', 'world!']\n",
        "\n",
        "WordPunctTokenizer: This tokenizer splits all punctuation into separate tokens. It's more aggressive in separating punctuation than word_tokenize.\n",
        "\n",
        "Example: text = \"Hello, world!\" might become ['Hello', ',', 'world', '!']\n",
        "\n",
        "MWETokenizer (Multi-Word Expression Tokenizer): This tokenizer is used to treat sequences of words as single tokens. For example, if you want \"New York\" to be considered one token instead of two, you would use this tokenizer after initial word tokenization.\n",
        "\n",
        "Example: If you define \"New York\" as an MWE, \"I live in New York City.\" would yield ['I', 'live', 'in', 'New York', 'City', '.']"
      ],
      "metadata": {
        "id": "YWoyFMr0Iq8P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t4tZZ_-oIEJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization"
      ],
      "metadata": {
        "id": "Smg72xTvIwKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is the process of reducing words to their base or root form, known as a lemma. Unlike stemming, which often chops off prefixes and suffixes and might result in a word that isn't a true dictionary word, lemmatization uses a vocabulary and morphological analysis to return the dictionary form of a word. For example, the words \"running\", \"runs\", and \"ran\" would all be lemmatized to \"run\". It's often used in natural language processing to ensure that different inflected forms of a word are treated as the same item, which can improve the accuracy of tasks like text classification and information retrieval."
      ],
      "metadata": {
        "id": "d2q4a3WyI7r-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Show me how to perform lemmatization in NLTK\n",
        "Certainly! Here's how you can perform lemmatization using NLTK:\n",
        "\n",
        "The lemmatization example executed successfully!\n",
        "\n",
        "From the output of cell 5282132f:\n",
        "\n",
        "Original words: ['The', 'cats', 'were', 'running', 'quickly', 'and', 'ate', 'mice', '.']\n",
        "\n",
        "Lemmatized words: ['The', 'cat', 'were', 'running', 'quickly', 'and', 'ate', 'mouse', '.']\n",
        "\n",
        "Notice that 'cats' was lemmatized to 'cat', and 'mice' to 'mouse'. 'running' remained 'running' because, by default, lemmatize treats words as nouns, and 'running' as a noun (e.g., 'the running of the bulls') doesn't change.\n",
        "Lemmatization with POS tags: {'running': 'run', 'better': 'good', 'ran': 'run', 'geese': 'goose'}\n",
        "\n",
        "Here, when we specify pos='v' for 'running' and 'ran', they correctly lemmatize to 'run'.\n",
        "'better' with pos='a' (adjective) correctly lemmatizes to 'good'.\n",
        "'geese' with pos='n' (noun) correctly lemmatizes to 'goose'.\n",
        "This demonstrates the importance of providing the correct Part-of-Speech (POS) tag for more accurate lemmatization, especially for words that can act as different parts of speech.\n",
        "\n",
        "We've covered quite a few fundamental NLP concepts in this notebook:\n",
        "\n",
        "Sentence Tokenization: We learned how to split text into individual sentences using NLTK's sent_tokenize function, understanding its reliance on the Punkt tokenizer.\n",
        "Word Tokenization: We explored different methods for breaking sentences into words:\n",
        "word_tokenize for general-purpose word tokenization.\n",
        "RegexpTokenizer for custom, regex-based tokenization, which we used to extract only alphanumeric words.\n",
        "We also discussed TreebankWordTokenizer, WhitespaceTokenizer, WordPunctTokenizer, and MWETokenizer for different use cases.\n",
        "Lemmatization: We dove into reducing words to their base form (lemma) using NLTK's WordNetLemmatizer, contrasting it with stemming and demonstrating the importance of Part-of-Speech (POS) tagging for accurate results."
      ],
      "metadata": {
        "id": "ZZnpqYg_JRMN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xqP6d_zGIzDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10651c59",
        "outputId": "023e5a39-0865-4c52-9c90-ec39560d0433"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the WordNet corpus, which is required for WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5282132f",
        "outputId": "8f5236fb-a9ce-4a7b-92cf-3e2574b23d9c"
      },
      "source": [
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "text = \"The cats were running quickly and ate mice.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Perform lemmatization on each word\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Lemmatized words:\", lemmas)\n",
        "\n",
        "# Example with different parts of speech (pos)\n",
        "# WordNetLemmatizer can take an optional 'pos' argument (part-of-speech)\n",
        "# 'n' for noun (default), 'v' for verb, 'a' for adjective, 'r' for adverb\n",
        "\n",
        "word_to_lemma = {\n",
        "    \"running\": lemmatizer.lemmatize(\"running\", pos=\"v\"),\n",
        "    \"better\": lemmatizer.lemmatize(\"better\", pos=\"a\"),\n",
        "    \"ran\": lemmatizer.lemmatize(\"ran\", pos=\"v\"),\n",
        "    \"geese\": lemmatizer.lemmatize(\"geese\", pos=\"n\")\n",
        "}\n",
        "\n",
        "print(\"\\nLemmatization with POS tags:\", word_to_lemma)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['The', 'cats', 'were', 'running', 'quickly', 'and', 'ate', 'mice', '.']\n",
            "Lemmatized words: ['The', 'cat', 'were', 'running', 'quickly', 'and', 'ate', 'mouse', '.']\n",
            "\n",
            "Lemmatization with POS tags: {'running': 'run', 'better': 'good', 'ran': 'run', 'geese': 'goose'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0218a8ae"
      },
      "source": [
        "In this example:\n",
        "\n",
        "1.  We import `WordNetLemmatizer` from `nltk.stem` and `word_tokenize` from `nltk.tokenize`.\n",
        "2.  We download the necessary `wordnet` and `omw-1.4` (Open Multilingual WordNet) corpora, which `WordNetLemmatizer` uses.\n",
        "3.  We initialize an instance of `WordNetLemmatizer`.\n",
        "4.  We tokenize a sample sentence into individual words.\n",
        "5.  We then iterate through the words and apply `lemmatizer.lemmatize()` to each word. By default, `lemmatize` treats words as nouns. You can specify the part of speech using the `pos` argument for more accurate results (e.g., `pos='v'` for verbs, `pos='a'` for adjectives)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic Lemmatization"
      ],
      "metadata": {
        "id": "lPkbYJEaJbSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the WordNet corpus, which is required for WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words=[\"cats\",\"running\",\"better\",\"ran\",\"geese\",\"flies\"]\n",
        "for word in words:\n",
        "  print(word,\"->\",lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyaMqOEUJooL",
        "outputId": "4a3d6548-2951-4980-f0f6-3b42ee1be61d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cats -> cat\n",
            "running -> running\n",
            "better -> better\n",
            "ran -> ran\n",
            "geese -> goose\n",
            "flies -> fly\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization with POS tags"
      ],
      "metadata": {
        "id": "Ni6hgkw0KJRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
        "print(lemmatizer.lemmatize(\"ran\", pos=\"v\"))\n",
        "print(lemmatizer.lemmatize(\"geese\", pos=\"n\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9NRP25aJz2D",
        "outputId": "d676f192-f7ff-4e5e-df79-23e571b12920"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "good\n",
            "run\n",
            "goose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization of a Sentence"
      ],
      "metadata": {
        "id": "fGTU4gquKd3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "text = \"The cats were running quickly and ate mice.\"\n",
        "words = word_tokenize(text)\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40gWZNASKZx9",
        "outputId": "f7abfd74-64c0-4e17-e9f6-311211c6815b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'cat', 'were', 'running', 'quickly', 'and', 'ate', 'mouse', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EAk7MU6nKpg9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "3Y13XmChNJVX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"This wonderful day is full of sunshine and joy. I love spending time outdoors.\""
      ],
      "metadata": {
        "id": "ZIKVqrkzMOQd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "f2_TOSulMUvd"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pol=[]\n",
        "for sentence in sentences:\n",
        "  blob=TextBlob(sentence)\n",
        "  pol.append(blob.sentiment.polarity)"
      ],
      "metadata": {
        "id": "KyiGseLKMXSb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=sum(pol)"
      ],
      "metadata": {
        "id": "FGPcEvidNug6"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob2=TextBlob(paragraph)\n",
        "blob2.sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNGzMUc-McXy",
        "outputId": "062490f8-d38c-4bb3-d911-3c5d3302b71f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sentence(\"This wonderful day is full of sunshine and joy.\"),\n",
              " Sentence(\"I love spending time outdoors.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=sum(pol)"
      ],
      "metadata": {
        "id": "IOkxHcGeMgmu"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if result >= 1:\n",
        "  print(\"Positive\")\n",
        "elif result == 0:\n",
        "  print(\"Neutral\")\n",
        "else:\n",
        "  print(\"Negative\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz6SsZzAMoJy",
        "outputId": "dd879056-bf39-4b83-e4ac-40c9c82ed886"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization function in ML"
      ],
      "metadata": {
        "id": "Lumo8j7wN7Zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text):\n",
        "  import spacy\n",
        "  nlp=spacy.load('en_core_web_sm')\n",
        "  doc=nlp(text)\n",
        "  lemmatized_text=' '.join([token.lemma_ for token in doc])\n",
        "  return lemmatized_text\n",
        "text=\"Students are studying natural language processing.\"\n",
        "lemmatized_text=lemmatize_text(text)\n",
        "print(lemmatized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl1t7WJeN-m0",
        "outputId": "f95fb33e-fcda-4f55-ee7a-e13a338feb04"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "student be study natural language processing .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mGF5gRZrOMTK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}